{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XmA47b1Uktg"
      },
      "source": [
        "В моей курсовой я использую датасеты на русском для задач анализа тональности и детекции токсичности. Я решила попробовать посчитать на них статистику, а именно: у меня есть идея, что токсичные тексты и тексты с негативной тональностью будут более похожи, чем все остальные. Возможно, это будет работать и в обратную сторону - не токсичные тексты и положительные, но мне кажется, что токсичности и негативность более маркированы.\n",
        "\n",
        "\n",
        "**Итого имеем:**\n",
        "\n",
        "Нулевая гипотеза: тексты с лэйблами Bad + 1 *будут не больше похожи*, чем тексты с лэйблами Bad + 0, Good + 1, Good + 0\n",
        "\n",
        "Альтернативная гипотеза: тексты с лэйблами Bad + 1 будут *более похожи*, чем тексты с лэйблами Bad + 0, Good + 1, Good + 0\n",
        "\n",
        "\n",
        "Про сами датасеты:\n",
        "\n",
        "[Датасет для анализа тональности](https://huggingface.co/datasets/ai-forever/kinopoisk-sentiment-classification):\n",
        "* Объем: 10500 текстов\n",
        "* Соотношение классов: 3500:3500:3500\n",
        "* Классы: Good, Bad, Neutral\n",
        "* Состав: отзывы на фильмы из Кинопоиска\n",
        "\n",
        "[Датасет для детекции токсичности](https://www.kaggle.com/datasets/blackmoon/russian-language-toxic-comments):\n",
        "* Объем: 14412 текста\n",
        "* Соотношение классов: 4826:9586\n",
        "* Классы: 0, 1\n",
        "* Состав: комментарии из Двача и Пикабу"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6QvC-B3bxVjZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0d7776a-a965-4d6f-e514-388119c0c0b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets==2.16.1 in /usr/local/lib/python3.11/dist-packages (2.16.1)\n",
            "Requirement already satisfied: fsspec==2023.6.0 in /usr/local/lib/python3.11/dist-packages (2023.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets==2.16.1) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets==2.16.1) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.16.1) (18.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.11/dist-packages (from datasets==2.16.1) (0.7)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.16.1) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets==2.16.1) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets==2.16.1) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets==2.16.1) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets==2.16.1) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets==2.16.1) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets==2.16.1) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.11/dist-packages (from datasets==2.16.1) (0.32.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets==2.16.1) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets==2.16.1) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.16.1) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.16.1) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.16.1) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.16.1) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.16.1) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.16.1) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets==2.16.1) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.4->datasets==2.16.1) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.4->datasets==2.16.1) (1.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.16.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.16.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.16.1) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets==2.16.1) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.16.1) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.16.1) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets==2.16.1) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.16.1) (1.17.0)\n",
            "Collecting ru-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.8.0/ru_core_news_sm-3.8.0-py3-none-any.whl (15.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m115.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pymorphy3>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ru-core-news-sm==3.8.0) (2.0.3)\n",
            "Requirement already satisfied: dawg2-python>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0) (0.9.0)\n",
            "Requirement already satisfied: pymorphy3-dicts-ru in /usr/local/lib/python3.11/dist-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0) (2.4.417150.4580142)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets==2.16.1 fsspec==2023.6.0\n",
        "!python -m spacy download ru_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xJyi6neqvNJD"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from collections import Counter, defaultdict\n",
        "import spacy\n",
        "import multiprocessing\n",
        "from tqdm import tqdm\n",
        "from scipy.stats import chi2_contingency\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IdBlDOGKPl7P"
      },
      "source": [
        "Импортирую датасеты"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kEUOZUJZwneS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f034af4-c68f-411f-d969-be2c1c3ab8fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/russian-language-toxic-comments\n"
          ]
        }
      ],
      "source": [
        "path = kagglehub.dataset_download(\"blackmoon/russian-language-toxic-comments\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PE6JqPNiw9gy"
      },
      "outputs": [],
      "source": [
        "tox_raw = pd.read_csv('/root/.cache/kagglehub/datasets/blackmoon/russian-language-toxic-comments/versions/1/labeled.csv')\n",
        "tox_raw = tox_raw"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EC9T6qErPpfC"
      },
      "source": [
        "Этот датасет поделен на train, val и test по дефолту, нам трейна хватит"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aG9OVdCjxAYR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d43c4b0-c6be-439f-a245-bccdd11e9790"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "sent_raw = load_dataset(\"ai-forever/kinopoisk-sentiment-classification\")\n",
        "sent_raw = sent_raw['train']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFruVxHCP-ki"
      },
      "source": [
        "Создаю частотные словари для сентимента"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hq02ZDlbzfd3",
        "outputId": "16efe9f4-454f-4cdb-efca-bf0718da5787"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Сохранён файл: freq_sent/freq_sent_label_Good.csv (строк: 42444)\n",
            "Сохранён файл: freq_sent/freq_sent_label_Bad.csv (строк: 47433)\n",
            "Сохранён файл: freq_sent/freq_sent_label_Neutral.csv (строк: 45814)\n",
            "Топ лемм для класса Good:\n",
            "фильм: 16533\n",
            "человек: 5397\n",
            "жизнь: 3633\n",
            "хороший: 3277\n",
            "герой: 2986\n",
            "время: 2436\n",
            "главный: 2419\n",
            "актёр: 2415\n",
            "смотреть: 2319\n",
            "роль: 2309\n",
            "картина: 2204\n",
            "история: 2179\n",
            "первый: 1990\n",
            "сказать: 1986\n",
            "кино: 1912\n",
            "раз: 1828\n",
            "мир: 1792\n",
            "сюжет: 1773\n",
            "год: 1740\n",
            "любовь: 1686\n",
            "Топ лемм для класса Bad:\n",
            "фильм: 18639\n",
            "хороший: 3374\n",
            "человек: 3022\n",
            "смотреть: 2674\n",
            "актёр: 2643\n",
            "герой: 2617\n",
            "сюжет: 2475\n",
            "кино: 2407\n",
            "сказать: 2379\n",
            "главный: 2266\n",
            "первый: 2213\n",
            "ни: 2206\n",
            "время: 2123\n",
            "игра: 2013\n",
            "говорить: 1805\n",
            "раз: 1735\n",
            "картина: 1704\n",
            "режиссёр: 1651\n",
            "зритель: 1576\n",
            "просмотр: 1575\n",
            "Топ лемм для класса Neutral:\n",
            "фильм: 15655\n",
            "человек: 3570\n",
            "хороший: 3107\n",
            "герой: 2648\n",
            "жизнь: 2258\n",
            "главный: 2080\n",
            "сказать: 2051\n",
            "смотреть: 2037\n",
            "актёр: 1962\n",
            "первый: 1922\n",
            "время: 1912\n",
            "кино: 1853\n",
            "сюжет: 1853\n",
            "картина: 1681\n",
            "роль: 1663\n",
            "игра: 1640\n",
            "ни: 1610\n",
            "раз: 1560\n",
            "посмотреть: 1492\n",
            "история: 1458\n"
          ]
        }
      ],
      "source": [
        "# Загружаем модель spaCy\n",
        "nlp = spacy.load(\"ru_core_news_sm\", disable=[\"ner\", \"parser\"])\n",
        "nlp.max_length = 2000000  # на всякий случай\n",
        "\n",
        "# Предобработка одной пачки\n",
        "def process_batch(batch):\n",
        "    results = []\n",
        "    docs = list(nlp.pipe(batch[\"text\"], batch_size=64, n_process=multiprocessing.cpu_count()))\n",
        "    for doc in docs:\n",
        "        lemmas = [\n",
        "            token.lemma_.lower()\n",
        "            for token in doc\n",
        "            if token.is_alpha and not token.is_stop\n",
        "        ]\n",
        "        results.append(lemmas)\n",
        "    return {\"lemmas\": results}\n",
        "\n",
        "\n",
        "# Применяем к датасету\n",
        "ds_with_lemmas = sent_raw.map(\n",
        "    process_batch,\n",
        "    batched=True,\n",
        "    batch_size=64,\n",
        "    num_proc=multiprocessing.cpu_count(),\n",
        "    desc=\"Лемматизация\",\n",
        ")\n",
        "\n",
        "# Строим частотные словари по лейблам\n",
        "label_freqs_sent = defaultdict(Counter)\n",
        "\n",
        "for example in ds_with_lemmas:\n",
        "    label = example[\"label_text\"]\n",
        "    label_freqs_sent[label].update(example[\"lemmas\"])\n",
        "\n",
        "# это я сохраняю результаты\n",
        "output_dir = \"freq_sent\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for lbl, counter in label_freqs_sent.items():\n",
        "    # Превращаем Counter в DataFrame\n",
        "    df = pd.DataFrame(counter.items(), columns=[\"lemma\", \"frequency\"])\n",
        "    # Сортируем по убыванию частоты\n",
        "    df = df.sort_values(by=\"frequency\", ascending=False).reset_index(drop=True)\n",
        "    # Сохраняем в CSV\n",
        "    out_path = os.path.join(output_dir, f\"freq_sent_label_{lbl}.csv\")\n",
        "    df.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
        "    print(f\"Сохранён файл: {out_path} (строк: {len(df)})\")\n",
        "\n",
        "# Пример: вывести топ-20 лемм\n",
        "print(\"Топ лемм для класса Good:\")\n",
        "for lemma, freq in label_freqs_sent['Good'].most_common(20):\n",
        "    print(f\"{lemma}: {freq}\")\n",
        "print(\"Топ лемм для класса Bad:\")\n",
        "for lemma, freq in label_freqs_sent['Bad'].most_common(20):\n",
        "    print(f\"{lemma}: {freq}\")\n",
        "print(\"Топ лемм для класса Neutral:\")\n",
        "for lemma, freq in label_freqs_sent['Neutral'].most_common(20):\n",
        "    print(f\"{lemma}: {freq}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XnbeM5LW9LK"
      },
      "source": [
        "Пипипупу.\n",
        "\n",
        "У положительных и негативных отзывов частотные слова практически одинаковые. Это странно, но у меня и в курсаче подозрительно низкое качество на нем выходило, я планировала проверить датасет. Вот и проверила:)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mD8lBGTaQFJX"
      },
      "source": [
        "То же самое для тональности"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D1R-Kcum8_ra"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "# проверить равномерность распределения\n",
        "# === ЗАГРУЗКА CSV ===\n",
        "texts = tox_raw['comment'].tolist()\n",
        "labels = tox_raw['toxic'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JHBxldR8ib0",
        "outputId": "8eb4adff-9f6c-45ab-8967-c92733e34bfc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 226/226 [03:00<00:00,  1.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Сохранён файл: freq_tox/freq_tox_label_1.0.csv (строк: 19917)\n",
            "Сохранён файл: freq_tox/freq_tox_label_0.0.csv (строк: 28584)\n",
            "\n",
            "Топ-20 лемм для класса 1.0:\n",
            "человек: 331\n",
            "хохол: 241\n",
            "год: 194\n",
            "говорить: 181\n",
            "хороший: 164\n",
            "тупой: 155\n",
            "русский: 155\n",
            "раз: 155\n",
            "тред: 142\n",
            "знать: 142\n",
            "делать: 135\n",
            "нахуй: 129\n",
            "смотреть: 124\n",
            "думать: 121\n",
            "россия: 120\n",
            "видеть: 119\n",
            "ни: 117\n",
            "писать: 110\n",
            "сделать: 109\n",
            "дело: 109\n",
            "\n",
            "Топ-20 лемм для класса 0.0:\n",
            "год: 1311\n",
            "человек: 786\n",
            "раз: 687\n",
            "время: 579\n",
            "хороший: 518\n",
            "работать: 497\n",
            "знать: 477\n",
            "делать: 445\n",
            "деньга: 439\n",
            "работа: 415\n",
            "говорить: 371\n",
            "два: 351\n",
            "первый: 350\n",
            "случай: 347\n",
            "дело: 343\n",
            "день: 341\n",
            "новый: 339\n",
            "стоить: 335\n",
            "сделать: 316\n",
            "сказать: 312\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# === ИНИЦИАЛИЗАЦИЯ spaCy ===\n",
        "nlp = spacy.load(\"ru_core_news_sm\", disable=[\"ner\", \"parser\"])\n",
        "nlp.max_length = 2_000_000\n",
        "\n",
        "# === ХРАНИЛИЩЕ ДЛЯ ЧАСТОТ ===\n",
        "label_freqs_tox = defaultdict(Counter)\n",
        "\n",
        "# === ОБРАБОТКА БАТЧАМИ ===\n",
        "def yield_batches(texts, labels, batch_size):\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        yield texts[i:i+batch_size], labels[i:i+batch_size]\n",
        "\n",
        "for text_batch, label_batch in tqdm(yield_batches(texts, labels, batch_size), total=len(texts)//batch_size + 1):\n",
        "    docs = list(nlp.pipe(text_batch, batch_size=64, n_process=multiprocessing.cpu_count()))\n",
        "    for doc, label in zip(docs, label_batch):\n",
        "        lemmas = [\n",
        "            token.lemma_.lower()\n",
        "            for token in doc\n",
        "            if token.is_alpha and not token.is_stop\n",
        "        ]\n",
        "        label_freqs_tox[label].update(lemmas)\n",
        "\n",
        "output_dir = \"freq_tox\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "for lbl, counter in label_freqs_tox.items():\n",
        "    # Превращаем Counter в DataFrame\n",
        "    df = pd.DataFrame(counter.items(), columns=[\"lemma\", \"frequency\"])\n",
        "    # Сортируем по убыванию частоты\n",
        "    df = df.sort_values(by=\"frequency\", ascending=False).reset_index(drop=True)\n",
        "    # Сохраняем в CSV\n",
        "    out_path = os.path.join(output_dir, f\"freq_tox_label_{lbl}.csv\")\n",
        "    df.to_csv(out_path, index=False, encoding=\"utf-8\")\n",
        "    print(f\"Сохранён файл: {out_path} (строк: {len(df)})\")\n",
        "\n",
        "# === ВЫВОД ТОП-20 ЛЕММ ДЛЯ КАЖДОГО КЛАССА ===\n",
        "for lbl, counter in label_freqs_tox.items():\n",
        "    print(f\"\\nТоп-20 лемм для класса {lbl}:\")\n",
        "    for lemma, freq in counter.most_common(20):\n",
        "        print(f\"{lemma}: {freq}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WAFWW_EOXZXt"
      },
      "source": [
        "Ну с тональностью частотные списки выглядят очень даже неплохо"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufKr0cLdQNdo"
      },
      "source": [
        "Делаю частотные списки по датасетам в общем, без деления по лэйблам"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxVbEhuFNmoM",
        "outputId": "71119e93-bc8a-40c6-aa3c-c0e3967e20c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sent\n",
            "фильм: 50827\n",
            "человек: 11989\n",
            "хороший: 9758\n",
            "герой: 8251\n",
            "жизнь: 7085\n",
            "смотреть: 7030\n",
            "актёр: 7020\n",
            "главный: 6765\n",
            "время: 6471\n",
            "сказать: 6416\n",
            "кино: 6172\n",
            "первый: 6125\n",
            "сюжет: 6101\n",
            "картина: 5589\n",
            "роль: 5343\n",
            "игра: 5325\n",
            "ни: 5324\n",
            "раз: 5123\n",
            "история: 4739\n",
            "говорить: 4694\n",
            "посмотреть: 4639\n",
            "просмотр: 4563\n",
            "год: 4470\n",
            "зритель: 4362\n",
            "режиссёр: 4201\n",
            "знать: 4168\n",
            "мир: 3984\n",
            "конец: 3910\n",
            "персонаж: 3848\n",
            "видеть: 3630\n",
            "сцена: 3624\n",
            "момент: 3565\n",
            "сделать: 3553\n",
            "интересный: 3425\n",
            "часть: 3398\n",
            "любовь: 3241\n",
            "понравиться: 3223\n",
            "слово: 3215\n",
            "стоить: 3215\n",
            "два: 3072\n",
            "дело: 3014\n",
            "новый: 3005\n",
            "снять: 2982\n",
            "думать: 2964\n",
            "работа: 2826\n",
            "место: 2823\n",
            "образ: 2809\n",
            "ребёнок: 2724\n",
            "увидеть: 2706\n",
            "получиться: 2668\n",
            "играть: 2619\n",
            "делать: 2619\n",
            "понять: 2587\n",
            "друг: 2545\n",
            "идея: 2538\n",
            "общий: 2500\n",
            "любить: 2473\n",
            "второй: 2469\n",
            "сыграть: 2463\n",
            "показать: 2457\n",
            "плохой: 2351\n",
            "актёрский: 2311\n",
            "понимать: 2265\n",
            "смысл: 2224\n",
            "книга: 2217\n",
            "музыка: 2211\n",
            "глаз: 2206\n",
            "экран: 2186\n",
            "каждый: 2183\n",
            "сценарий: 2173\n",
            "чувство: 2146\n",
            "лицо: 2143\n",
            "взгляд: 2133\n",
            "последний: 2073\n",
            "хотеться: 2050\n",
            "смешной: 2016\n",
            "минута: 1990\n",
            "сторона: 1973\n",
            "юмор: 1937\n",
            "впечатление: 1936\n",
            "душа: 1929\n",
            "происходить: 1922\n",
            "являться: 1919\n",
            "идти: 1895\n",
            "красивый: 1880\n",
            "совершенно: 1860\n",
            "шедевр: 1857\n",
            "снимать: 1847\n",
            "вопрос: 1832\n",
            "больший: 1817\n",
            "жить: 1813\n",
            "итог: 1802\n",
            "день: 1791\n",
            "создатель: 1790\n",
            "остаться: 1770\n",
            "деньга: 1749\n",
            "полный: 1748\n",
            "настолько: 1726\n",
            "большой: 1719\n",
            "сказка: 1693\n",
            "Tox\n",
            "год: 1505\n",
            "человек: 1117\n",
            "раз: 842\n",
            "хороший: 682\n",
            "время: 674\n",
            "знать: 619\n",
            "делать: 580\n",
            "работать: 568\n",
            "говорить: 552\n",
            "деньга: 505\n",
            "работа: 458\n",
            "дело: 452\n",
            "первый: 439\n",
            "два: 435\n",
            "сделать: 425\n",
            "думать: 419\n",
            "ни: 418\n",
            "сказать: 415\n",
            "случай: 399\n",
            "день: 399\n",
            "стоить: 394\n",
            "новый: 385\n",
            "место: 380\n",
            "видеть: 378\n",
            "страна: 376\n",
            "смотреть: 370\n",
            "ребёнок: 365\n",
            "проблема: 362\n",
            "идти: 362\n",
            "жить: 338\n",
            "жизнь: 335\n",
            "цена: 322\n",
            "вопрос: 308\n",
            "россия: 305\n",
            "город: 302\n",
            "интересный: 299\n",
            "час: 295\n",
            "какой: 285\n",
            "нормальный: 285\n",
            "дом: 283\n",
            "месяц: 282\n",
            "машина: 278\n",
            "рубль: 269\n",
            "нужный: 260\n",
            "понимать: 258\n",
            "слово: 254\n",
            "хохол: 245\n",
            "последний: 245\n",
            "писать: 243\n",
            "пост: 242\n",
            "найти: 242\n",
            "пара: 241\n",
            "купить: 240\n",
            "разный: 237\n",
            "второй: 229\n",
            "мир: 225\n",
            "хотеть: 225\n",
            "спасибо: 225\n",
            "часть: 224\n",
            "понять: 223\n",
            "игра: 223\n",
            "считать: 221\n",
            "фильм: 221\n",
            "вода: 218\n",
            "система: 216\n",
            "получить: 213\n",
            "сторона: 210\n",
            "дать: 207\n",
            "посмотреть: 199\n",
            "русский: 198\n",
            "тип: 197\n",
            "тред: 196\n",
            "факт: 193\n",
            "написать: 192\n",
            "три: 191\n",
            "читать: 190\n",
            "больший: 190\n",
            "помнить: 190\n",
            "брать: 190\n",
            "получать: 189\n",
            "ситуация: 189\n",
            "рука: 187\n",
            "давать: 187\n",
            "взять: 186\n",
            "закон: 185\n",
            "начать: 184\n",
            "платить: 184\n",
            "пойти: 182\n",
            "телефон: 182\n",
            "остаться: 181\n",
            "покупать: 180\n",
            "интернет: 180\n",
            "сша: 180\n",
            "тупой: 177\n",
            "магазин: 177\n",
            "момент: 176\n",
            "уровень: 174\n",
            "дешёвый: 174\n",
            "обычный: 173\n",
            "дорогой: 173\n"
          ]
        }
      ],
      "source": [
        "total_freq_sent = Counter()\n",
        "for counter in label_freqs_sent.values():\n",
        "    total_freq_sent.update(counter)\n",
        "\n",
        "total_freq_tox = Counter()\n",
        "for counter in label_freqs_tox.values():\n",
        "    total_freq_tox.update(counter)\n",
        "# Теперь total_freq — это единый Counter со всеми леммами из всех лэйблов\n",
        "top_n = 100  # например, 100 самых частотных\n",
        "print('Sent')\n",
        "for lemma, freq in total_freq_sent.most_common(top_n):\n",
        "    print(f\"{lemma}: {freq}\")\n",
        "print('Tox')\n",
        "for lemma, freq in total_freq_tox.most_common(top_n):\n",
        "    print(f\"{lemma}: {freq}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8LAvqORBFMt",
        "outputId": "31d91b09-92e2-466e-e1d4-afe7c9ff8d28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "207014\n",
            "1616936\n"
          ]
        }
      ],
      "source": [
        "print(sum(total_freq_tox.values()))\n",
        "print(sum(total_freq_sent.values()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGIABKSUQgFe"
      },
      "source": [
        "Так как абсолютные частоты по датасетам сильно разнятся, посчитаем относительные"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "Lp1xR3xr46ka"
      },
      "outputs": [],
      "source": [
        "label_freqs_tox_per = {}\n",
        "for k, v in label_freqs_tox.items():\n",
        "  v = {key : value/sum(total_freq_tox.values()) for key, value in v.items()}\n",
        "  v = v = {k: v for k, v in sorted(v.items(), key=lambda item: item[1], reverse=True)}\n",
        "  label_freqs_tox_per[k] = v\n",
        "\n",
        "label_freqs_sent_per = {}\n",
        "for k, v in label_freqs_sent.items():\n",
        "  v = {key : value/sum(total_freq_sent.values()) for key, value in v.items()}\n",
        "  v = {k: v for k, v in sorted(v.items(), key=lambda item: item[1], reverse=True)}\n",
        "  label_freqs_sent_per[k] = v\n",
        "\n",
        "total_freq_tox_per = {}\n",
        "for k, v in total_freq_tox.items():\n",
        "    total_freq_tox_per[k] = v/sum(total_freq_tox.values())\n",
        "total_freq_tox_per = {k: v for k, v in sorted(total_freq_tox_per.items(), key=lambda item: item[1], reverse=True)}\n",
        "\n",
        "total_freq_sent_per = {}\n",
        "for k, v in total_freq_sent.items():\n",
        "    total_freq_sent_per[k] = v/sum(total_freq_tox.values())\n",
        "total_freq_sent_per = {k: v for k, v in sorted(total_freq_sent_per.items(), key=lambda item: item[1], reverse=True)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sENsUFcRId4"
      },
      "source": [
        "Функция для подсчета меры Шайкевича\n",
        "\n",
        "Я решила попробовать убрать слово \"фильм\", потому что оно овер частотно в датасете для сентимента и портит картину. Заодно убрала еще три самых частотных везде, но это картину не поменяло все равно."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Rx-RW5Jxsi83"
      },
      "outputs": [],
      "source": [
        "def compute_shaykevich_metric(corpus1_counts, corpus2_counts, top_n=1000):\n",
        "    all_lemmas = set(corpus1_counts) | set(corpus2_counts)\n",
        "    combined_freq = {\n",
        "        lemma: corpus1_counts.get(lemma, 0) + corpus2_counts.get(lemma, 0)\n",
        "        for lemma in all_lemmas\n",
        "    }\n",
        "\n",
        "    top_lemmas = sorted(combined_freq.items(), key=lambda x: x[1], reverse=True)[:top_n]\n",
        "    top_words = [lemma for lemma, _ in top_lemmas ]\n",
        "\n",
        "    top_words.remove('фильм')\n",
        "    top_words.remove('человек')\n",
        "    top_words.remove('хороший')\n",
        "    top_words.remove('год')\n",
        "\n",
        "    sum_min = 0\n",
        "    sum_avg = 0\n",
        "    for lemma in top_words:\n",
        "        f1 = corpus1_counts.get(lemma, 0)\n",
        "        f2 = corpus2_counts.get(lemma, 0)\n",
        "        sum_min += min(f1, f2)\n",
        "        sum_avg += (f1 + f2) / 2\n",
        "\n",
        "    if sum_avg == 0:\n",
        "        return 0.0\n",
        "\n",
        "    return sum_min / sum_avg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjBWpCESRMsk"
      },
      "source": [
        "Считаем меры для разных сочетаний: первые две - по датасетам в целом, дальще комбинируем датасеты по лэйблам. Например, sent_tox_bad_1 - берем список из сентимента по лэйблу Bad и список из токсичности по лэйблу 1.0\n",
        "\n",
        "Для сравнения я решила игнорить нейтральные отзывы в сентименте, хотя можно было бы, например, объединить их с положительными, хотя не думаю, что это сильно бы что-то поменяло"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "M0frPQKn_JDv"
      },
      "outputs": [],
      "source": [
        "sent_good_bad = compute_shaykevich_metric(label_freqs_sent_per['Good'], label_freqs_sent_per['Bad'])\n",
        "tox_yes_no = compute_shaykevich_metric(label_freqs_tox_per[1.0], label_freqs_tox_per[0.0])\n",
        "sent_tox_bad_1 = compute_shaykevich_metric(label_freqs_sent_per['Bad'], label_freqs_tox_per[1.0])\n",
        "sent_tox_bad_0 = compute_shaykevich_metric(label_freqs_sent_per['Bad'], label_freqs_tox_per[0.0])\n",
        "sent_tox_good_1 = compute_shaykevich_metric(label_freqs_sent_per['Good'], label_freqs_tox_per[1.0])\n",
        "sent_tox_good_0 = compute_shaykevich_metric(label_freqs_sent_per['Good'], label_freqs_tox_per[0.0])\n",
        "sent_tox_all = compute_shaykevich_metric(total_freq_sent_per, total_freq_tox_per)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JCryLTu-TM-Z",
        "outputId": "1a9c8692-5718-43a5-9d26-963e3f17acf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tox_yes_no: 0.7941970264052487\n",
            "sent_good_bad: 0.7941970264052487\n",
            "sent_good_bad_1: 0.503091870382508\n",
            "sent_tox_bad_0: 0.4959720588944889\n",
            "sent_tox_good_1: 0.477793607294367\n",
            "sent_tox_good_0: 0.4831549767627885\n",
            "sent_tox_all: 0.1502995471231234\n"
          ]
        }
      ],
      "source": [
        "print(f\"\"\"tox_yes_no: {sent_good_bad}\n",
        "sent_good_bad: {sent_good_bad}\n",
        "sent_good_bad_1: {sent_tox_bad_1}\n",
        "sent_tox_bad_0: {sent_tox_bad_0}\n",
        "sent_tox_good_1: {sent_tox_good_1}\n",
        "sent_tox_good_0: {sent_tox_good_0}\n",
        "sent_tox_all: {sent_tox_all}\"\"\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59Ke3MJ-S49b"
      },
      "source": [
        "Уже итак видно, что токсичные и негативные тексты не более похожи между собой, чем все остальные, но посчитаем еще хи-квадрат:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v03Zr9_dGQAM",
        "outputId": "b5b7a240-a51d-4496-e410-f2b358246bbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "χ² = 0.00000, p-value = 1.00000, degree of freedom = 1, expected frequencies = [[0.49998012 0.49908381]\n",
            " [0.48090535 0.48004323]]\n"
          ]
        }
      ],
      "source": [
        "chi2, p, dof, expected = chi2_contingency([[sent_tox_bad_1, sent_tox_bad_0],\n",
        "                                           [sent_tox_good_1, sent_tox_good_0]])\n",
        "print(f\"χ² = {chi2:.5f}, p-value = {p:.5f}, degree of freedom = {dof}, expected frequencies = {expected}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Попробуем убрать поправку Йейтса, чтобы у нас не занулялся хи-квадрат"
      ],
      "metadata": {
        "id": "OCTGiSj0nwnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chi2, p, dof, expected = chi2_contingency([[sent_tox_bad_1, sent_tox_bad_0],\n",
        "                                           [sent_tox_good_1, sent_tox_good_0]], correction=False)\n",
        "print(f\"χ² = {chi2:.5f}, p-value = {p:.5f}, degree of freedom = {dof}, expected frequencies = {expected}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJ7eqgN_m-ow",
        "outputId": "b1055016-6101-4d6c-8abd-57aaa40376df"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "χ² = 0.00008, p-value = 0.99291, degree of freedom = 1, expected frequencies = [[0.49998012 0.49908381]\n",
            " [0.48090535 0.48004323]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSIXMqI8YNIO"
      },
      "source": [
        "На всякий случай посчитала еще через кастомную функцию"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlmosCSxXwQO",
        "outputId": "d40bb144-35ae-491b-ec0f-802101e63ba8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "χ² = 0.0000395\n"
          ]
        }
      ],
      "source": [
        "def chi_square(a, b, c, d):\n",
        "    \"\"\"Вычисляет хи-квадрат для 2x2 таблицы частот\"\"\"\n",
        "    N = a + b + c + d\n",
        "    E1 = (a + c) * (a + b) / N  # ожидаемое значение для ячейки a\n",
        "    E2 = (a + c) * (c + d) / N  # ожидаемое значение для ячейки c\n",
        "\n",
        "    chi2 = ((a - E1) ** 2) / E1 + ((c - E2) ** 2) / E2\n",
        "    return chi2\n",
        "\n",
        "a = sent_tox_bad_1\n",
        "b = sent_tox_bad_0\n",
        "c = sent_tox_good_1\n",
        "d = sent_tox_good_0\n",
        "\n",
        "chi2_value = chi_square(a, b, c, d)\n",
        "print(f\"χ² = {chi2_value:.7f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fw05UXvqZi-O"
      },
      "source": [
        "Пороговое значнение хи-квадрата, после которого мы можем отвергнуть нулевую гипотезу для первой степени свободы равно 3.841. Наше значение хи-квадрата практически равно нулю, и естественно, нулевую гипотезу мы не отвергаем."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SL9hSQm3YWaQ"
      },
      "source": [
        "Я просчиталась, но где?\n",
        "\n",
        "Во-первых, с датасетом по сентименту действительно что-то не так, но это я уже для курсовой буду смотреть. Во-вторых, я взяла один датасет тематический - отзывы на фильмы, а другой - просто комменты про все. Это ожидаемо сместило частотные списки в сторону лексики, связанной с кино. Ну и еще, я думаю, что можно было бы взять слова, частотные по всем лэйблам для каждого датасета и выкинуть их. Как, например, я сделала со словом \"фильм\".\n",
        "\n",
        "Ну а на основе тех данных, которые есть, можно сделать вывод, что лексика в токсичных и негативных текстах не более близка, чем у всех остальных"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}